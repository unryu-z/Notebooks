# Python3爬虫的各种库

使用anaconda建立python 3.7 的环境

## 请求库

实现http请求操作：

- **requests**库：阻塞式http请求库，发出一个请求后程序会一直等待服务器响应，直到响应结束后程序才进行下一步处理。
- **Selenium**库：自动化测试工具，可以驱动浏览器执行特定的动作。(需Chrome配合**ChromeDriver**或Foxfire配合GeckoDriver或PhantomJS)
- 安装**ChromeDrive**、**PhantomJS**，注意配置环境变量
- **aiohttp**库：提供异步web服务。(需要安装其它依赖库)
  - **cchardet**：字符编码检测库
  - **aiodns**：加速DNS解析库

在cmd命令行窗口先进入新建的环境，再运行（或者再anaconda中安装库）

```
conda install requests selenium aiohttp
pip3 install cchardet
```

**检验安装**

```
>>> import requests
>>> import selenium
>>> import aiohttp
```

```
chromedriver
```



## 解析库的安装

- lxml：HTML和XML解析库，支持XPath解析方式
- BeautifulSoup4：HTML和XML解析库，可以从网页中提取数据
- pyquery：网页解析工具，支持CSS选择器
- tesserocr：OCR识别库（先安装**软件tesseract**，**注意**软件版本和库的版本对应，安装后Tesseract-OCR下tessdata文件夹要复制到python文件夹下（和Lib文件同一级））

```
conda install lxml beautifulsoup4 pyquery
conda install -c simonflueckiger tesserocr Pillow
```

**检验安装**

```
>>> import lxml
>>> from bs4 import BeautifulSoup # 注意BeautifulSoup4包的名称为bs4
>>> soup = BeautifulSoup('<p>Hello</p>','lxml')
>>> print(soup.p.string) # 输出Hello表面安装成功
>>> import pyquery
>>> import tesserocr
>>> print(tesserocr.file_to_text("E:\Program\PycharmProjects\pythonspider\image.png"))

```

```
tesseract F:\Users\ZOU\Pictures\image.png result -l eng && cat result.txt
```



## 数据库软件

关系数据库**MySQL**(需要VS2019)和非关系数据库**MongoDB**、**Redis**

## 储存库

- **PyMySQL**：通过PyMySQL库，将数据储存到MySQL中
- **PyMongo**：通过PyMongo库，将数据储存到MongoDB中
- **redis-py**：与Redis交互
- **RedisDump**：需要先安装Ruby+MSYS2(**安装路径不能有空格**)

```
conda install pymysql pymongo redis-py
pip3 install redis
# ? pip3 install redis-py
gem install redis-dump
```

**验证安装**

```
>>> import pymysql
>>> pymysql.VERSION
(0, 10, 1, None)
>>> import pymongo
>>> pymongo.version
'3.11.0'
>>> import redis
>>> redis.__version__
'3.5.3'
```

**错误**：

```
C:\Users\ZOU>redis-dump
ERROR (Errno::ENOENT): No such file or directory - ps -o rss= -p 6644
```

修改文件dump.rb，文件路径D:\ProgramData\Ruby27-x64\lib\ruby\gems\2.7.0\gems\redis-dump-0.4.0\lib\redis，使用#号注释第32行：

```
#        `ps -o rss= -p #{Process.pid}`.to_i # in kb
```

## web库

- **Flask**：做API服务，使用Flask+Redis维护动态代理池和Cookies池
- **Tornado**：利用Tornado+Redis来搭建一个ADSL拨号代理池

```
conda install flask tornado
```

**验证安装**

```
>>> from flask import Flask
>>> app = Flask(__name__)
... @app.route("/")
... def hello():
...    return "Hello World"

>>> if __name__== "__main__":
...     app.run()
...     
 * Serving Flask app "<input>" (lazy loading)
 * Environment: production
   WARNING: This is a development server. Do not use it in a production deployment.
   Use a production WSGI server instead.
 * Debug mode: off
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit) # 系统将在5000端口开启Web服务
```

打开：http://127.0.0.1:5000/

```
>>> import tornado.ioloop
>>> import tornado.web
>>> class MainHandler(tornado.web.RequestHandler):
...     def get(self):
...         self.write("hello, world")
...         
>>> def make_app():
...     return tornado.web.Application([
...         (r"/", MainHandler),
...     ])
... 
>>> if __name__ == "__main__":
...     app = make_app()
...     app.listen(8888)
...     tornado.ioloop.IOLoop.current().start()
...     
WARNING:tornado.access:404 GET /favicon.ico (127.0.0.1) 2.00ms # 表示没有标签图标
```

打开：http://127.0.0.1:8888/

## App爬取相关库

- 安装**软件Charles**，配置SSL证书（用于HTTPS抓包），PC和手机都要**安装证书**
- **mitmproxy库**：支持HTTP和HTTPS的抓包程序，通过控制台运行；再**安装证书未进行**
- <u>**Appium安装**：**未进行(IOS必须配合Mac)**</u>

```
pip3 install mitmproxy
```

## 爬虫框架的安装

- **pyspider**：依赖于PhantonJS，支持JavaScript渲染，需安装**PyCurl库**（https://www.lfd.uci.edu/~gohlke/pythonlibs/#pycurl选择适合python的版本）
- **Scrapy**：爬虫框架
- **Scrapy-Splash**：需要先安装**软件Docker**，通过Docker安装Splash服务；在安装Scrapy-Splash的python库
- **Scrapy-Redis**：Scrapy的分布式扩展模板

```
pip3 install pycurl-7.43.0.4-cp37-cp37m-win_amd64.whl
pip3 install pyspider
conda install scrapy
pip3 install scrapy-splash
pip3 install scrapy-redis
```

**pyspider验证安装** https://blog.csdn.net/weixin_39190382/article/details/104923254

```
(python3spider) E:\Program\PycharmProjects\pythonspider>pyspider all
Traceback (most recent call last):
  File "d:\programdata\anaconda3\envs\python3spider\lib\runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "d:\programdata\anaconda3\envs\python3spider\lib\runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "D:\ProgramData\Anaconda3\envs\python3spider\Scripts\pyspider.exe\__main__.py", line 4, in <module>
  File "d:\programdata\anaconda3\envs\python3spider\lib\site-packages\pyspider\run.py", line 231
    async=True, get_object=False, no_input=False):
        ^
SyntaxError: invalid syntax
```

**async为python 3.7自留关键字**，我将在以下三个文件夹中其替换为zasync

- python\lib\site-packages\pyspider\run.py
- python\lib\site-packages\pyspider\fetcher\tornado_fetcher.py
- lib\site-packages\pyspider\webui\app.py

```
(python3spider) E:\Program\PycharmProjects\pythonspider>pyspider all
d:\programdata\anaconda3\envs\python3spider\lib\site-packages\pyspider\libs\utils.py:196: FutureWarning: timeout is not supported on your platform.
  warnings.warn("timeout is not supported on your platform.", FutureWarning)
[W 201122 18:17:05 run:413] phantomjs not found, continue running without it.
[I 201122 18:17:07 result_worker:49] result_worker starting...
[I 201122 18:17:08 processor:211] processor starting...
[I 201122 18:17:08 scheduler:647] scheduler starting...
[I 201122 18:17:08 scheduler:586] in 5m: new:0,success:0,retry:0,failed:0
[I 201122 18:17:09 scheduler:782] scheduler.xmlrpc listening on 127.0.0.1:23333
[I 201122 18:17:09 tornado_fetcher:638] fetcher starting...
```

上面有一个warning，是因为Windows不支持timeout，**忽略**就行

对于错误 **phantomjs not found, continue running without it** ，将phantomjs.exe放在python.exe同一文件目录下。

再者，始终停在fetcher starting，因为wsgidav版本过高，降为2.4.1版

```
pip3 uninstall wsgidav
python -m pip install wsgidav==2.4.1
```

有发现错误：**ImportError: cannot import name DispatcherMiddleware**，werkzeug版本将到0.16.1版

```
python -m pip uninstall werkzeug
python -m pip install werkzeug==0.16.1
```

**Scrapy验证安装**

```
(python3spider) E:\Program\PycharmProjects\pythonspider>scrapy
Scrapy 2.4.0 - no active project

Usage:
  scrapy <command> [options] [args]

Available commands:
  bench         Run quick benchmark test
  commands
  fetch         Fetch a URL using the Scrapy downloader
  genspider     Generate new spider using pre-defined templates
  runspider     Run a self-contained spider (without creating a project)
  settings      Get settings values
  shell         Interactive scraping console
  startproject  Create new project
  version       Print Scrapy version
  view          Open URL in browser, as seen by Scrapy

  [ more ]      More commands available when run from project directory

Use "scrapy <command> -h" to see more info about a command
```

启动Docker错误**Error response from daemon: open \.\pipe\docker_engine_linux: The system cannot find the file specified**，使用下面命令：

```
C:\Windows\system32>Net stop com.docker.service
Docker Desktop Service 服务正在停止.
Docker Desktop Service 服务已成功停止。


C:\Windows\system32>Net start com.docker.service
Docker Desktop Service 服务正在启动 .
Docker Desktop Service 服务已经启动成功。
```



## 远程部署相关库（未进行）

分布式爬虫需要将一份代码同时部署在多台主机上来协同运行。

一种是Scrapy有一个扩展组件Scrapyd，可以实现远程管理Scrapy任务，包括部署源码、启动任务、监听任务等。

另一种是通过Docker集群部署，需要将爬虫制作为Docker镜像，只需在主机安装Docker，就可以直接运行爬虫。