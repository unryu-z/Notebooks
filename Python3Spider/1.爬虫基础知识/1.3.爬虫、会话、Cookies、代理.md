# 爬虫的基本原理

## 爬虫概述

### 获取网页

爬虫首先要做的工作就是获取网页，这里就是获取网页的源代码。构造一个请求并发送给服务器，然后接收到响应并将其解析出来。

Python 提供了许多库来帮助我们实现这个操作，如 urllib、requests 等。我们可以用这些库来帮助我们实现 HTTP 请求操作，请求和响应都可以用类库提供的数据结构来表示，得到响应之后只需要解析数据结构中的 Body 部分即得到网页的源代码。

### 提取信息

获取网页源代码后，接下来就是分析网页源代码，从中提取我们想要的数据。首先，最通用的方法便是采用正则表达式提取，这是一个万能的方法，但是在**构造正则表达式时比较复杂且容易出错**

另外，由于网页的结构有一定的规则，所以还有一些根据网页节点属性、CSS 选择器或 XPath 来提取网页信息的库，如 Beautiful Soup、pyquery、lxml 等。使用这些库，我们可以高效快速地从中提取网页信息，如节点的属性、文本值等。

### 保存数据

提取信息后，我们一般会将提取到的数据保存到某处以便后续使用。这里保存形式有多种多样，如可以简单保存为 TXT 文本或 JSON 文本，也可以保存到数据库，如 MySQL 和 MongoDB 等，也可保存至远程服务器，如借助 SFTP 进行操作等

### 自动化程序

自动化程序可以在抓取过程中进行各种异常处理、错误重试等操作，确保爬取持续高效地运行

## 爬取怎样的数据

在网页中我们能看到各种各样的信息，最常见的便是常规网页，它们对应着 HTML 代码，而最常抓取的便是 **HTML 源代码**

另外，可能有些网页返回的不是 HTML 代码，而是一个 **JSON 字符串**（其中 API 接口大多采用这样的形式），这种格式的数据方便传输和解析，它们同样可以抓取，而且数据提取更加方便。

此外，我们还可以看到各种**二进制数据**，如图片、视频和音频等。利用爬虫，我们可以将这些二进制数据抓取下来，然后保存成对应的文件名。

另外，还可以看到**各种扩展名的文件**，如 CSS、JavaScript 和配置文件等，这些其实也是最普通的文件，只要在浏览器里面可以访问到，就可以将其抓取下来。

上述内容其实都对应各自的 URL，是基于 HTTP 或 HTTPS 协议的，只要是这种数据，爬虫都可以抓取

## JavaScript 渲染页面

有时候，我们在用 urllib 或 requests 抓取网页时，得到的**源代码实际和浏览器中看到的不一样**

这是一个非常常见的问题。现在网页越来越多地采用 Ajax、前端模块化工具来构建，整个网页可能都是由 **JavaScript 渲染**出来的，也就是说原始的 HTML 代码就是一个空壳，例如：

```
<!DOCTYPE html>
<html>
    <head>
        <meta charset="UTF-8">
        <title>This is a Demo</title>
    </head>
    <body>
        <div id="container">
        </div>
    </body>
    <script src="app.js"></script>
</html>
```

`body` 节点里面只有一个 `id` 为 `container` 的节点，但是需要注意在 `body` 节点后**引入了 app.js**，它便负责整个网站的渲染。

在浏览器中打开这个页面时，**首先**会加载这个 HTML 内容，接着浏览器会发现其中**引入**了一个 app.js 文件，然后便会接着去**请求**这个文件，获取到该文件后，便会**执行**其中的 JavaScript 代码，而 JavaScript 则会**改变 HTML 中的节点，向其添加内容**，最后得到完整的页面。

但是在用 urllib 或 requests 等库请求当前页面时，我们得到的只是这个 HTML 代码，它不会帮助我们去继续加载这个 JavaScript 文件，这样也就看不到浏览器中的内容了。

因此，使用基本 HTTP 请求库得到的源代码可能跟浏览器中的页面源代码不太一样。对于这样的情况，我们可以**分析其后台 Ajax 接口**，也可使用 Selenium、Splash 这样的库来实现**模拟 JavaScript 渲染**。

后面，我们会详细介绍如何采集 JavaScript 渲染的网页。

# 会话和Cookies

## 静态网页和动态网页

先了解一下静态网页和动态网页的概念。这里还是前面的示例代码

这是最基本的 HTML 代码，我们将其保存为一个.html 文件，然后把它放在某台具有固定公网 IP 的主机上，**主机上装上 Apache 或 Nginx 等服务器**，这样这台主机就可以作为服务器了，其他人便可以通过访问服务器看到这个页面，这就搭建了一个最简单的网站。

这种网页的内容是 HTML 代码编写的，文字、图片等内容均通过写好的 HTML 代码来指定，这种页面叫作**静态网页**。它加载速度快，编写简单，但是存在很大的缺陷，如可维护性差，不能根据 URL 灵活多变地显示内容等。例如，我们想要给这个网页的 URL 传入一个 `name` 参数，让其在网页中显示出来，是无法做到的。

因此，**动态网页**应运而生，它可以**动态解析 URL 中参数的变化**，**关联数据库**并**动态呈现不同的页面内容**，非常灵活多变。我们现在遇到的大多数网站都是动态网站，它们不再是一个简单的 HTML，而是可能由 JSP、PHP、Python 等语言编写的，其功能比静态网页强大和丰富太多了。

此外，动态网站还可以实现用户登录和注册的功能。

## 无状态 HTTP

在了解会话和 Cookies 之前，我们还需要了解 HTTP 的一个特点，叫作无状态。

HTTP 的无状态是指 **HTTP 协议对事务处理是没有记忆能力的**，也就是说<u>服务器不知道客户端是什么状态</u>。当我们向服务器发送请求后，服务器解析此请求，然后返回对应的响应，服务器负责完成这个过程，而且这个过程是完全独立的，服务器不会记录前后状态的变化，也就是缺少状态记录。这意味着如果后续需要处理前面的信息，则必须重传，这导致<u>需要额外传递一些前面的重复请求</u>，才能获取后续响应，然而这种效果显然不是我们想要的。为了保持前后状态，我们肯定不能将前面的请求全部重传一次，这太浪费资源了，对于这种需要用户登录的页面来说，更是棘手。

这时两个用于**保持 HTTP 连接状态**的技术就出现了，它们分别是会话和 Cookies。**会话在服务端**，也就是网站的服务器，用来保存用户的会话信息；**Cookies 在客户端**，也可以理解为浏览器端，有了 Cookies，浏览器在下次访问网页时会自动附带上它发送给服务器，服务器通过识别 Cookies 并鉴定出是哪个用户，然后再判断用户是否是登录状态，然后返回对应的响应。

我们可以理解为 Cookies 里面保存了登录的凭证，有了它，只需要在下次请求携带 Cookies 发送请求而不必重新输入用户名、密码等信息重新登录了。

### 会话

在 Web 中，会话对象用来存储特定用户会话所需的属性及配置信息。这样，当用户在应用程序的 Web 页之间跳转时，存储在会话对象中的变量将不会丢失，而是在整个用户会话中一直存在下去。当用户请求来自应用程序的 Web 页时，如果该用户还没有会话，则 Web 服务器将自动创建一个会话对象。**当会话过期或被放弃后，服务器将终止该会话**。

### Cookies

Cookies 指某些网站为了**辨别用户身份**、**进行会话跟踪**而存储在用户本地终端上的数据。

#### 会话维持

那么，我们怎样利用 Cookies 保持状态呢？

当客户端**第一次请求**服务器时，服务器会返回一个请求头中带有 **`Set-Cookie` 字段**的响应给客户端，用来标记是哪一个用户，客户端浏览器会把 Cookies 保存起来。当浏览器**下一次再请求**该网站时，浏览器会把此 **Cookies 放到请求头**一起提交给服务器，**Cookies 携带了会话 ID 信息**，服务器检查该 Cookies 即可找到对应的会话是什么，然后再判断会话来以此来辨认用户状态。

在成功登录某个网站时，服务器会告诉客户端**设置哪些 Cookies 信息**，在**后续访问**页面时客户端会把 Cookies 发送给服务器，服务器再找到对应的会话加以判断。

如果会话中的某些设置登录状态的变量是有效的，那就证明用户处于登录状态，此时返回登录之后才可以查看的网页内容，浏览器再进行解析便可以看到了。反之，如果传给服务器的 Cookies 是无效的，或者会话已经过期了，我们将不能继续访问页面，此时可能会收到错误的响应或者跳转到登录页面重新登录。

所以，Cookies 和会话需要配合，一个处于客户端，一个处于服务端，二者共同协作，就实现了登录会话控制。

#### 属性结构

接下来，我们来看看 **Cookies 都有哪些内容**。这里以知乎为例，在浏览器开发者工具中打开 Application 选项卡，然后在左侧会有一个 Storage 部分，最后一项即为 Cookies，将其点开就可以看到Cookies，可能不只一个。

可以看到，这里有很多条目，其中**每个条目可以称为 Cookie**。它有如下几个属性。

- **Name**：该 **Cookie 的名称**。一旦创建，该名称便不可更改。
- **Value**：该 **Cookie 的值**。如果值为 **Unicode 字符**，需要为字符编码。如果值为**二进制数据**，则需要使用 BASE64 编码。
- **Domain**：可以访问该 Cookie 的**域名**。例如，如果设置为.zhihu.com，则所有以 zhihu.com，结尾的域名都可以访问该 Cookie
- **Max Age**：该 Cookie **失效的时间**，单位为**秒**，也常和 **Expires** 一起使用，通过它可以计算出其有效时间。Max Age 如果为正数，则该 Cookie 在 Max Age 秒之后失效。如果为负数，则关闭浏览器时 Cookie 即失效，浏览器也不会以任何形式保存该 Cookie
- **Path**：该 Cookie 的使用**路径**。如果**设置为 /path/**，则只有路径为 /path/ 的页面可以访问该 Cookie。如果**设置为 /**，则本域名下的所有页面都可以访问该 Cookie
- **Size 字段**：此 Cookie 的**大小**
- **HTTP 字段**：Cookie 的 **`httponly` 属性**。若此属性为 `true`，则只有在 HTTP 头中会带有此 Cookie 的信息，而不能通过 `document.cookie` 来访问此 Cookie
- **Secure**：该 Cookie 是否仅被使用**安全协议传输**。安全协议有 HTTPS 和 SSL 等，在网络上传输数据之前先将数据加密。**默认为 `false`**。

#### 会话 Cookie 和持久 Cookie

从表面意思来说，会话 Cookie 就是把 Cookie 放在浏览器内存里，浏览器在关闭之后该 Cookie 即失效；持久 Cookie 则会保存到客户端的硬盘中，下次还可以继续使用，用于长久保持用户登录状态。

其实严格来说，没有会话 Cookie 和持久 Cookie 之分，只是由 Cookie 的 Max Age 或 Expires 字段决定了过期的时间。

因此，一些持久化登录的网站其实就是把 Cookie 的有效时间和会话有效期设置得比较长，下次我们再访问页面时仍然携带之前的 Cookie，就可以直接保持登录状态。

## 常见误区

在谈论会话机制的时候，常常听到这样一种误解 “只要关闭浏览器，会话就消失了”，这种理解是错误的。可以想象一下会员卡的例子，除非顾客主动对店家提出销卡，否则店家绝对不会轻易删除顾客的资料。对会话来说，也是一样，**除非程序通知服务器删除一个会话，否则服务器会一直保留**。比如，程序一般都是在我们做注销操作时才去删除会话。

但是当我们关闭浏览器时，浏览器不会主动在关闭之前通知服务器它将要关闭，所以服务器根本不会有机会知道浏览器已经关闭。之所以会有这种错觉，是因为大部分会话机制都使用会话 Cookie 来保存会话 ID 信息，而关闭浏览器后 Cookies 就消失了，再次连接服务器时，也就无法找到原来的会话了。如果服务器设置的 Cookies 保存到硬盘上，或者使用某种手段改写浏览器发出的 HTTP 请求头，把原来的 Cookies 发送给服务器，则再次打开浏览器，仍然能够找到原来的会话 ID，依旧还是可以保持登录状态的。

而且恰恰是由于关闭浏览器不会导致会话被删除，这就需要服务器为会话设置一个失效时间，当距离客户端上一次使用会话的时间超过这个失效时间时，服务器就可以认为客户端已经停止了活动，才会把会话删除以节省存储空间。

# 代理的基本原理

我们在做爬虫的过程中经常会遇到这样的情况，最初爬虫正常运行，正常抓取数据，一切看起来都是那么美好，然而一杯茶的功夫可能就会出现错误，比如 403 Forbidden，这时候打开网页一看，可能会看到 “您的 IP 访问频率太高” 这样的提示。出现这种现象的原因是网站采取了一些**反爬虫措施**。比如，服务器会检测某个 IP 在单位时间内的请求次数，如果超过了这个阈值，就会直接拒绝服务，返回一些错误信息，这种情况可以称为封 IP。

既然服务器检测的是某个 IP 单位时间的请求次数，那么借助某种方式来伪装我们的 IP，让服务器识别不出是由我们本机发起的请求，不就可以成功防止封 IP 了吗？

一种有效的方式就是使用代理，后面会详细说明代理的用法。在这之前，需要先了解下代理的基本原理，它是怎样实现 IP 伪装的呢？

## 基本原理

代理实际上指的就是**代理服务器**(proxy server)，它的功能是代理网络用户去取得网络信息。形象地说，它是网络信息的中转站。在我们正常请求一个网站时，是发送了请求给 Web 服务器，Web 服务器把响应传回给我们。如果设置了代理服务器，实际上就是在本机和服务器之间搭建了一个桥，此时本机不是直接向 Web 服务器发起请求，而是向代理服务器发出请求，请求会发送给代理服务器，然后由代理服务器再发送给 Web 服务器，接着由代理服务器再把 Web 服务器返回的响应转发给本机。这样我们同样可以正常访问网页，但这个过程中 Web 服务器识别出的真实 IP 就不再是我们本机的 IP 了，就成功实现了 IP 伪装，这就是代理的基本原理。

## 代理的作用

那么，代理有什么作用呢？我们可以简单列举如下。

- 突破自身 IP 访问限制，访问一些平时不能访问的站点。
- 访问一些单位或团体内部资源：比如使用教育网内地址段免费代理服务器，就可以用于对教育网开放的各类 FTP 下载上传，以及各类资料查询共享等服务。
- 提高访问速度：通常代理服务器都设置一个较大的硬盘缓冲区，当有外界的信息通过时，同时也将其保存到缓冲区中，当其他用户再访问相同的信息时，则直接由缓冲区中取出信息，传给用户，以提高访问速度。
- 隐藏真实 IP：上网者也可以通过这种方法隐藏自己的 IP，免受攻击。对于爬虫来说，我们用代理就是为了隐藏自身 IP，防止自身的 IP 被封锁。

## 爬虫代理

对于爬虫来说，由于爬虫爬取速度过快，在爬取过程中可能遇到同一个 IP 访问过于频繁的问题，此时网站就会让我们输入验证码登录或者直接封锁 IP，这样会给爬取带来极大的不便。

使用代理隐藏真实的 IP，让服务器误以为是代理服务器在请求自己。这样在爬取过程中通过**不断更换代理**，就不会被封锁，可以达到很好的爬取效果。

## 代理分类

代理分类时，既可以根据协议区分，也可以根据其匿名程度区分。

### 根据协议区分

根据代理的协议，代理可以分为如下类别。

- **FTP 代理服务器**：主要用于**访问 FTP 服务器**，一般有上传、下载以及缓存功能，端口一般为 21、2121 等。
- **HTTP 代理服务器**：主要用于**访问网页**，一般有内容过滤和缓存功能，端口一般为 80、8080、3128 等。
- **SSL/TLS 代理**：主要用于**访问加密网站**，一般有 SSL 或 TLS 加密功能（最高支持 128 位加密强度），端口一般为 443。
- **RTSP 代理**：主要用于访问 **Real 流媒体服务器**，一般有缓存功能，端口一般为 554。
- **Telnet 代理**：主要用于 **telnet 远程控制**（黑客入侵计算机时常用于隐藏身份），端口一般为 23。
- **POP3/SMTP 代理**：主要用于 POP3/SMTP 方式**收发邮件**，一般有缓存功能，端口一般为 110/25。
- **SOCKS 代理**：只是单纯**传递数据包**，不关心具体协议和用法，所以速度快很多，一般有缓存功能，端口一般为 1080。SOCKS 代理协议又分为 SOCKS4 和 SOCKS5，前者只支持 TCP，而后者支持 TCP 和 UDP，还支持各种身份验证机制、服务器端域名解析等。简单来说，SOCK4 能做到的 SOCKS5 都可以做到，但 SOCKS5 能做到的 SOCK4 不一定能做到。

### 根据匿名程度区分

根据代理的匿名程度，代理可以分为如下类别。

- **高度匿名代理**：会将数据包原封不动地转发，在服务端看来就好像真的是一个普通客户端在访问，而记录的 IP 是代理服务器的 IP。
- **普通匿名代理**：会在数据包上做一些改动，服务端上有可能发现这是个代理服务器，也有一定几率追查到客户端的真实 IP。代理服务器通常会加入的 HTTP 头有 `HTTP_VIA` 和 `HTTP_X_FORWARDED_FOR`。
- **透明代理**：不但改动了数据包，还会告诉服务器客户端的真实 IP。这种代理除了能用缓存技术提高浏览速度，能用内容过滤提高安全性之外，并无其他显著作用，最常见的例子是内网中的硬件防火墙。
- **间谍代理**：指组织或个人创建的用于记录用户传输的数据，然后进行研究、监控等目的的代理服务器。

## 常见代理设置

- 使用网上的免费代理：最好使用高匿代理，另外可用的代理不多，需要在使用前筛选一下可用代理，也可以进一步维护一个代理池。
- 使用付费代理服务：互联网上存在许多代理商，可以付费使用，质量比免费代理好很多。
- **ADSL 拨号**：拨一次号换一次 IP，稳定性高，也是一种比较有效的解决方案。****