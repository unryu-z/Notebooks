# 使用urllib(request、error、parse)

https://docs.python.org/zh-cn/3/library/urllib.request.html?highlight=urllib#request-objects

在 Python 2 中，有 urllib 和 urllib2 两个库来实现请求的发送。而在 Python 3 中，已经不存在 urllib2 这个库了，统一为 urllib，其官方文档链接为：https://docs.python.org/3/library/urllib.html。

首先，了解一下 urllib 库，它是 Python 内置的 HTTP 请求库，不需要额外安装即可使用。它包含如下 **4 个模块**。

- **`response`**：
- **`request`**：它是最基本的 **HTTP 请求模块**，可以用来模拟发送请求。就像在浏览器里输入网址然后回车一样，只需要给库方法传入 URL 以及额外的参数，就可以模拟实现这个过程了。
- **`error`**：**异常处理模块**，如果出现请求错误，我们可以捕获这些异常，然后进行重试或其他操作以保证程序不会意外终止。
- **`parse`**：一个**工具模块**，提供了许多 URL 处理方法，比如**拆分**、**解析**、**合并**等。
- **`robotparser`**：主要是用来**识别网站的 robots.txt 文件**，然后判断哪些网站可以爬，哪些网站不可以爬，它其实用得比较少。

## 1. 发送请求 urllib.request

### urllib.request.urlopen()

**`urllib.request` 模块**提供了最基本的构造 HTTP 请求的方法，利用它可以模拟浏览器的一个请求发起过程，同时它还带有**处理授权验证**（authenticaton）、**重定向**（redirection)、**浏览器 Cookies** 以及其他内容。

以 Python 官网为例，利用以下代码爬取源代码：

```
import urllib.request # 导入urllib包的request模板

response = urllib.request.urlopen("https://www.python.org") # 请求并得到回应
print(response.read().decode('utf-8')) # 输出回应
```

- 调用 **`read()` 方法**可以得到**返回的网页内容**，并在python控制台输出源代码

- 利用**type()方法**输出响应的类型，可以知道**响应**是一个**http.client.HTTPResponse类型**的对象，

  - 主要包含 `read()`、`readinto()`、`getheader(name)`、`getheaders()`、`fileno()` 等**方法**
  - 主要包含 `msg`、`version`、`status`、`reason`、`debuglevel`、`closed` 等**属性**：

  ```
  >>> print(type(response)) # 利用type()方法输出响应的类型
  <class 'http.client.HTTPResponse'> # 是一个HTTPResponse类型的对象
  ```

- 调用 **`status` 属性**可以得到返回结果的状态码，如 200 代表请求成功，404 代表网页未找到等

- 调用 **`getheader()` 方法**并传递一个**参数 `Server`** 获取了响应头中的 `Server` 值，结果是 `nginx`，意思是服务器是用 Nginx 搭建的

```
>>> print(response.status) #返回结果的状态码
200
>>> print(response.getheaders()) # 响应的头信息
[('Connection', 'close'), ('Content-Length', '50834'), ('Server', 'nginx'), ('Content-Type', 'text/html; charset=utf-8'), ('X-Frame-Options', 'DENY'), ('Via', '1.1 vegur, 1.1 varnish, 1.1 varnish'), ('Accept-Ranges', 'bytes'), ('Date', 'Sat, 06 Mar 2021 06:58:02 GMT'), ('Age', '415'), ('X-Served-By', 'cache-bwi5146-BWI, cache-hkg17928-HKG'), ('X-Cache', 'HIT, HIT'), ('X-Cache-Hits', '1, 1438'), ('X-Timer', 'S1615013882.069168,VS0,VE0'), ('Vary', 'Cookie'), ('Strict-Transport-Security', 'max-age=63072000; includeSubDomains')]
>>> print(response.getheader('Server'))  # 调用 getheader() 方法并传递一个参数 Server 获取了响应头中的 Server 值
nginx
```

**`urlopen()` 函数的 API**：

```
urllib.request.urlopen(url, data=None, [timeout, ]*,
						cafile=None, capath=None, cadefault=False, context=None)
```

第一个参数可以传递 URL 之外，我们还可以传递其他内容，比如 `data`（附加数据）、`timeout`（超时时间）等

#### data参数

`data` 参数是可选的。如果要添加该参数，并且**如果**它是字节流编码格式的内容，即 **`bytes` 类型**，则需要通过 **`bytes()` 方法**转化。另外，如果传递了这个参数，则它的请求方式就**不再是 GET 方式**，**而是 POST 方式**。

实例：

```
>>> import urllib.parse  # 导入urllib包的parse模板
>>> import urllib.request  # 导入urllib包的request模板
>>> data = bytes(urllib.parse.urlencode({'word': 'hello'}), encoding='utf8')
... # urllib.parse模板中的urlencode()方法将 字典 转化为 字符串
... # bytes()将 str(字符串) 转化为 bytes(字节流) 类型
... # 参数encoding='utf8'指定编码格式
>>> response = urllib.request.urlopen('http://httpbin.org/post', data=data)
>>> print(response.read())
b'{\n  "args": {}, \n  "data": "", \n  "files": {}, \n  "form": {\n    "word": "hello"\n  }, \n  "headers": {\n    "Accept-Encoding": "identity", \n    "Content-Length": "10", \n    "Content-Type": "application/x-www-form-urlencoded", \n    "Host": "httpbin.org", \n    "User-Agent": "Python-urllib/3.7", \n    "X-Amzn-Trace-Id": "Root=1-60436b4c-31e702d9667bbdd44f6bc577"\n  }, \n  "json": null, \n  "origin": "113.14.161.88", \n  "url": "http://httpbin.org/post"\n}\n'

```

这里我们传递了一个**参数 `word`**，**值是 `hello`**。它需要被转码成 `bytes`（字节流）类型。其中转字节流采用了 **`bytes()` 方法**，该方法的第一个参数需要是 **`str`（字符串）类型**，需要用 `urllib.parse` 模块里的 **`urlencode()` 方法**来**将参数字典转化为字符串**；第二个参数`encoding='utf8'`指定**编码格式**，这里指定为 `utf8`。

这里请求的站点是 httpbin.org，它可以提供 HTTP 请求测试。本次我们请求的 URL 为 http://httpbin.org/post，这个链接可以用来<u>测试 POST 请求</u>，它可以输出请求的一些信息，其中包含我们传递的 `data` 参数。

结果即

```
b'{
	"args": {}, 
    "data": "", 
    "files": {}, 
    "form": {
    	"word": "hello"
    }, 
    "headers": {
    	"Accept-Encoding": "identity", 
        "Content-Length": "10", 
        "Content-Type": "application/x-www-form-urlencoded", 
        "Host": "httpbin.org", 
        "User-Agent": "Python-urllib/3.7", 
        "X-Amzn-Trace-Id": "Root=1-60436b4c-31e702d9667bbdd44f6bc577"
    }, 
    "json": null, 
    "origin": "113.14.161.88", 
    "url": "http://httpbin.org/post"
}
```

传递的参数出现在了 **`form` 字段**中，这表明是模拟了**表单**提交的方式，以 **POST 方式**传输数据

#### timeout参数

`timeout` 参数用于设置超时时间，单位为**秒**，意思就是如果请求超出了设置的这个时间，还没有得到响应，就会**抛出异常**。如果不指定该参数，就会使用全局默认时间。它支持 HTTP、HTTPS、FTP 请求。

实例：

```
>>> import urllib.request
>>> response = urllib.request.urlopen('http://httpbin.org/get', timeout=1)
>>> print(response.read())
b'{\n  "args": {}, \n  "headers": {\n    "Accept-Encoding": "identity", \n    "Host": "httpbin.org", \n    "User-Agent": "Python-urllib/3.7", \n    "X-Amzn-Trace-Id": "Root=1-604372e6-10df23c133374080371eaa7e"\n  }, \n  "origin": "113.14.161.88", \n  "url": "http://httpbin.org/get"\n}\n'
```

结果即：

```
{
	"args": {}, 
    "headers": {
    "Accept-Encoding": "identity", 
    "Host": "httpbin.org", 
    "User-Agent": "Python-urllib/3.7", 
    "X-Amzn-Trace-Id": "Root=1-604372e6-10df23c133374080371eaa7e"
    }, 
    "origin": "113.14.161.88", 
    "url": "http://httpbin.org/get"
}
```

若设置timeout=0.1，则服务器在0.1秒内没有响应，于是抛出了 `URLError` 异常。该异常属于 `urllib.error` 模块，错误原因是超时，结果如下：

```
During handling of the above exception, another exception occurred:

Traceback (most recent call last): File "/var/py/python/urllibtest.py", line 4, in <module> response = urllib.request.urlopen('http://httpbin.org/get', timeout=1)
...
urllib.error.URLError: <urlopen error timed out>
```

因此，可以通过设置这个超时时间来控制一个网页，如果长时间未响应，就跳过它的抓取。这可以利用 **`try except` 语句**来实现，相关代码如下：

```
>>> try:
...     response = urllib.request.urlopen('http://httpbin.org/get', timeout=0.01)
... except urllib.error.URLError as e:
...     if isinstance(e.reason, socket.timeout):
...         print('TIME OUT')
... 
TIME OUT
```

- 这里我们请求了 http://httpbin.org/get 测试链接，设置超时时间是 0.01 秒，然后**捕获了 `URLError` 异常**，接着**判断异常**是 `socket.timeout` 类型（意思就是超时异常），从而得出它确实是因为超时而报错，打印输出了 `TIME OUT`。

#### 其他参数

除了 `data` 参数和 `timeout` 参数外，还有 **`context` 参数**，它必须**是 `ssl.SSLContext` 类型**，用来**指定 SSL 设置**。

此外，**`cafile` 和 `capath`** 这两个参数分别**指定 CA 证书**和它的**路径**，这个在请求 HTTPS 链接时会有用。

**`cadefault` 参数**现在已经**弃用**了，其默认值为 `False`。

前面讲解了 `urlopen()` 方法的用法，通过这个最基本的方法，我们可以完成简单的请求和网页抓取。若需更加详细的信息，可以参见官方文档：https://docs.python.org/3/library/urllib.request.html。

### urllib.request.Request()

利用 `urlopen()` 方法可以实现最基本请求的发起，但这几个简单的参数并不足以构建一个完整的请求

如果请求中需要加入 Headers 等信息，可以利用更强大的 **`Request` 类**来构建具体的请求信息。

实例：

```
import urllib.request
request = urllib.request.Request('http://python.org')
response = urllib.request.urlopen(request)
print(response.read().decode('utf-8'))
```

- 依然是<u>用 `urlopen()` 方法来发送这个请求</u>，只不过这次该方法的<u>参数不再是 URL</u>，而是一个 **`Request` 类型**的对象。通过构造这个数据结构，一方面我们可以**将请求独立成一个对象**，另一方面**可更加丰富和灵活地配置参数**。

**Request()的API**：

```
class urllib.request.Request(url, data=None, headers={},
								origin_req_host=None, unverifiable=False, method=None)
```

- **参数 `url`** 用于请求 URL，这是必传参数，其他都是可选参数。

- **参数 `data`** 如果要传，必须传 **`bytes`（字节流）类型**的。如果它是字典，可以参考上面的`data`参数

- **参数 `headers`** 是一个**字典**，它就是**请求头**，它的具体参数可以看`HTTP基本原理`中的`请求头`，我们可以在构造请求时通过 `headers` 参数直接构造，也可以通过调用请求实例的 **`add_header()` 方法**添加。 添加请求头最常用的用法就是通过修改 `User-Agent` 来伪装浏览器，<u>默认的 `User-Agent` 是 Python-urllib</u>，我们可以通过修改它来<u>伪装浏览器</u>。比如要伪装火狐浏览器，你可以把它设置为：

  ```
  Mozilla/5.0 (X11; U; Linux i686) Gecko/20071127 Firefox/2.0.0.11
  ```

- **参数 `origin_req_host`** 指的是**请求方的 host 名称**或者 **IP 地址**。

- **参数 `unverifiable`** 表示这个请求是否是无法验证的，默认是 `False`，意思就是说<u>用户没有足够权限来选择接收这个请求的结果</u>。例如，我们请求一个 HTML 文档中的图片，但是我们没有自动抓取图像的权限，这时 unverifiable `的值就是` True`。

- **参数 `method`** 是一个**字符串**，用来指示**请求使用的方法**，比如 **GET**、**POST** 和 **PUT** 等。

实例：

```
>>> from urllib import request, parse
>>> url = 'http://httpbin.org/post'
>>> headers = {
...     'User-Agent': 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)',
...     'Host': 'httpbin.org'
... }  # 请求头
>>> dict = {
...     'name': 'Germey'
... }
>>> data = bytes(parse.urlencode(dict), encoding='utf8')  # 同urllib.parse.urlencode()
>>> req = request.Request(url=url, data=data, headers=headers, method='POST')
... # 请求方式为POST 同urllib.request.Request()
>>> response = request.urlopen(req)  # 发起请求 同urllib.request.urlopen()
>>> print(response.read().decode('utf-8'))
{
  "args": {}, 
  "data": "", 
  "files": {}, 
  "form": {
    "name": "Germey"
  }, 
  "headers": {
    "Accept-Encoding": "identity", 
    "Content-Length": "11", 
    "Content-Type": "application/x-www-form-urlencoded", 
    "Host": "httpbin.org", 
    "User-Agent": "Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)", 
    "X-Amzn-Trace-Id": "Root=1-604385e3-0e8708b14073d2c24ffd930b"
  }, 
  "json": null, 
  "origin": "113.14.161.88", 
  "url": "http://httpbin.org/post"
}
```

另外，请求头`headers` 也可以用 **`add_header()` 方法**来添加：

```
req = request.Request(url=url, data=data, method='POST')
req.add_header('User-Agent', 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)')
```

### 高级用法 (Hander、Opener)

设置请求中的Cookies 处理、代理设置等，需要用到工具 Handler ，可以把它理解为**各种处理器**，有专门处理登录验证的，有处理 Cookies 的，有处理代理设置的。利用它们，我们几乎可以做到 HTTP 请求中所有的事情。

首先，介绍一下 **`urllib.request` 模块**里的 **`BaseHandler` 类**，它是<u>所有其他 `Handler` 的父类</u>，它提供了最基本的方法，例如 、**`build_open()`、 `default_open()`、`protocol_request()`**等。

接下来，就有各种 `Handler` 子类**继承**这个 `BaseHandler` 类，举例如下：

- **`HTTPDefaultErrorHandler`**：用于处理 **HTTP 响应错误**，错误都会抛出 `HTTPError` 类型的异常。
- **`HTTPRedirectHandler`**：用于处理**重定向**。
- **`HTTPCookieProcessor`**：用于处理 **Cookies**。
- **`ProxyHandler`**：用于设置**代理**，默认代理为空。
- **`HTTPPasswordMgr`**：用于**管理密码**，它维护了用户名和密码的表。
- **`HTTPBasicAuthHandler`**：用于管理**认证**，如果一个链接打开时需要认证，那么可以用它来解决认证问题。

另外，还有其他的 `Handler` 类，详情可以参考官方文档：https://docs.python.org/3/library/urllib.request.html#urllib.request.BaseHandler。

另一个比较重要的类就是 **`OpenerDirector`**，我们可以称为 `Opener`。我们之前用过的 <u>`urlopen()` 就是 urllib 为我们提供的一个 `Opener`</u>。

那么，为什么要引入 `Opener` 呢？因为需要实现更高级的功能。之前使用的 `Request` 和 `urlopen()` 相当于类库为你封装好了极其常用的请求方法，利用它们可以完成基本的请求，但是现在不一样了，我们需要实现更高级的功能，所以需要深入一层进行配置，使用更底层的实例来完成操作，所以这里就用到了 `Opener`。

**`Opener` 可以使用 `open()` 方法**，返回的类型和 `urlopen()` 如出一辙。那么，它和 `Handler` 有什么关系呢？简而言之，就是**利用 `Handler` 来构建 `Opener`**。

#### 验证 `HTTPBasicAuthHandler()`

需要用到`urllib.request`模板

有些网站在打开时就会弹出提示框，直接提示你输入用户名和密码，**认证**成功后才能查看页面。那么，如果要请求这样的页面，该怎么办呢？借助 **`HTTPBasicAuthHandler` **就可以完成，相关代码如下：

```
from urllib.request import HTTPPasswordMgrWithDefaultRealm, HTTPBasicAuthHandler, build_opener
from urllib.error import URLError

username = 'username'  # 用户名
password = 'password'  # 密码
url = 'http://localhost:5000/'  # URL

p = HTTPPasswordMgrWithDefaultRealm()  # HTTPBasicAuthHandler的参数对象HTTPPasswordMgrWithDefaultRealm
p.add_password(None, url, username, password)  # 利用add_password()对URL添加用户名和密码
auth_handler = HTTPBasicAuthHandler(p)  # 实例化HTTPBasicAuthHandler对象
# 以上建立了一个处理验证的Handler
opener1 = build_opener(auth_handler)  # 使用build_opener()方法构建一个Opener 命名为opener1

try:
    result = opener1.open('http://localhost:5000/')  # 利用Opener的open()方法打开链接 即发送请求
    html = result.read().decode('utf-8')  # 利用read()方法获得网页源代码 并指定编码格式
    print(html)
except URLError as e:
    print(e.reason)
```

#### 代理 ProxyHandler()

需要用到`urllib.request`模板

使用代理，实例：

```
from urllib.error import URLError
from urllib.request import ProxyHandler, build_opener

proxy_handler = ProxyHandler({
    'http': 'http://127.0.0.1:9743',
    'https': 'https://127.0.0.1:9743'
})  # 建立一个设置代理的Handler 参数是字典
opener2 = build_opener(proxy_handler)
  # 使用build_opener()方法及上面构建的Handler构建一个Opener 命名为opener2
try:
    response = opener2.open('https://www.baidu.com')  # 利用Opener的open()方法打开链接 即发送请求
    print(response.read().decode('utf-8'))  # 利用read()方法获得网页源代码 并指定编码格式
except URLError as e:
    print(e.reason)
```

这里我们在<u>本地搭建了一个代理</u>，它运行在 9743 端口上。

这里使用了 `ProxyHandler`，其**参数**是一个**字典**，**键名**是协议类型（比如 HTTP 或者 HTTPS 等），**键值**是代理链接，可以添加多个代理。

然后，利用这个 Handler 及 `build_opener()` 方法构造一个 `Opener`，之后发送请求即可。

#### Cookies `HTTPCookieProcessor()`

需要用到`http.cookjar`和`urllib.request`模板

1. 获取网站的Cookies

   ```
   import http.cookiejar, urllib.request
   
   cookie = http.cookiejar.CookieJar()  # 创建一个CookieJar对象
   handler = urllib.request.HTTPCookieProcessor(cookie)
     # 利用HTTPCookieProcessor()构建一个处理Cookies的Handler
   opener3 = urllib.request.build_opener(handler)
     # 使用build_opener()方法及上面构建的Handler构建一个Opener 命名为opener3
   response = opener3.open('http://www.baidu.com')  # 利用Opener的open()方法打开链接 即发送请求
   for item in cookie:
       print(item.name + "=" + item.value)
   ```

   - 首先，必须声明一个 `CookieJar` 对象
   - 然后，利用 `HTTPCookieProcessor` 来构建一个 `Handler`
   - 然后，利用 `build_opener()` 方法构建出 `Opener`
   - 最后，执行 `open()` 函数即可

   运行结果如下，输出了每条 Cookie 的名称和值：

   ```
   BAIDUID=B8FC7EADF20BDF3D2673B94D229F44EE:FG=1
   BIDUPSID=B8FC7EADF20BDF3D4A782650F715D7A1
   H_PS_PSSID=33257_33273_33595_33570_33460_26350_22157
   PSTM=1615096589
   BDSVRTM=0
   BD_HOME=1
   ```

2. 将Cookies保存为文件，代码如下：

   ```
   filename = 'cookies.txt'
   cookie = http.cookiejar.MozillaCookieJar(filename)  # 指定创建对象的位置
   handler = urllib.request.HTTPCookieProcessor(cookie)
   opener4 = urllib.request.build_opener(handler)
   response = opener4.open('http://www.baidu.com')
   cookie.save(ignore_discard=True, ignore_expires=True)  # 保存Cookies
   ```

   第2行创建了一个**`MozillaCookieJar()`**对象，`CookieJar` 就换成 `MozillaCookieJar`，它是 `CookieJar` 的子类，在生成文件时会用到。可以用来处理 Cookies 和文件相关的事件，比如<u>读取和保存 Cookies</u>，可以将 Cookies 保存成 Mozilla 型浏览器的 Cookies 格式。

   运行之后，生成了一个cookies.txt文件：

   ```
   # Netscape HTTP Cookie File
   # http://curl.haxx.se/rfc/cookie_spec.html
   # This is a generated file!  Do not edit.
   
   .baidu.com	TRUE	/	FALSE	1646633892	BAIDUID	5A219A9F1DA0B727E2D9EB53BD2075C2:FG=1
   .baidu.com	TRUE	/	FALSE	3762581539	BIDUPSID	5A219A9F1DA0B72780BF36C9DCE98A77
   .baidu.com	TRUE	/	FALSE		H_PS_PSSID	33639_33344_33594_33570_26350_33652
   .baidu.com	TRUE	/	FALSE	3762581539	PSTM	1615097892
   www.baidu.com	FALSE	/	FALSE		BDSVRTM	5
   www.baidu.com	FALSE	/	FALSE		BD_HOME	1
   ```

   另外，**`LWPCookieJar`** 同样可以读取和保存 Cookies，但是保存的格式和 `MozillaCookieJar` 不一样，它会保存成 libwww-perl (LWP) 格式的 Cookies 文件，声明如下：

   ```
   cookie = http.cookiejar.LWPCookieJar(filename)
   ```

   结果如下：

   ```
   #LWP-Cookies-2.0
   Set-Cookie3: BAIDUID="FEC1C97D474B462090C37810C52EE7B6:FG=1"; path="/"; domain=".baidu.com"; path_spec; domain_dot; expires="2022-03-07 06:28:44Z"; comment=bd; version=0
   Set-Cookie3: BIDUPSID=FEC1C97D474B4620A28FC6321E087C97; path="/"; domain=".baidu.com"; path_spec; domain_dot; expires="2089-03-25 09:42:51Z"; version=0
   Set-Cookie3: H_PS_PSSID=33514_33260_33344_31254_33595_26350; path="/"; domain=".baidu.com"; path_spec; domain_dot; discard; version=0
   Set-Cookie3: PSTM=1615098524; path="/"; domain=".baidu.com"; path_spec; domain_dot; expires="2089-03-25 09:42:51Z"; version=0
   Set-Cookie3: BDSVRTM=0; path="/"; domain="www.baidu.com"; path_spec; discard; version=0
   Set-Cookie3: BD_HOME=1; path="/"; domain="www.baidu.com"; path_spec; discard; version=0
   ```

3. 读取Cookies文件

   调用**`cookie.load()`函**数读取本地Cookies文件，以LWP格式为例，代码如下：

   ```
   cookie = http.cookiejar.LWPCookieJar()
   cookie.load('cookies.txt', ignore_discard=True, ignore_expires=True)
   handler = urllib.request.HTTPCookieProcessor(cookie)
   opener5 = urllib.request.build_opener(handler)
   response = opener5.open('http://www.baidu.com')
   print(response.read().decode('utf-8'))
   ```

   - 首先，创建一个`LWPCookieJar`对象
   - 然后，调用 `load()` 方法，读取本地的 Cookies 文件，获取到了 Cookies 的内容；不过前提是我们首先生成了 LWPCookieJar 格式的 Cookies，并保存成文件
   - 然后，利用读取的 Cookies 使用同样的方法构建 Handler 和 Opener 即可完成操作
   - 最后，执行 `open()` 函数发起请求

## 2. 处理异常 urllib.error

urllib 的 `error` 模块定义了由 `request` 模块产生的异常。如果出现了问题，`request` 模块便会抛出 `error` 模块中定义的异常。

### URLError

URLError 类来自 urllib 库的 error 模块，它继承自 OSError 类，是 error 异常模块的基类，由 <u>request 模块生的异常都可以通过捕获这个类来处理</u>。它具有一个属性 **reason**，即返回**错误的原因**。

实例：

```
from urllib import request, error
try:
    response = request.urlopen('http://cuiqingcai.com/index.htm')
except error.URLError as e:
    print(e.reason)
```

打开不存在的网页，捕获了 **URLError** 这个异常，运行结果：

```
Not Found
```

程序没有直接报错，而是输出了如上内容。通过如上操作，我们就可以**避免程序异常终止**，同时异常得到了有效处理。

### HTTPError

`HTTPError`是 `URLError` 的子类，专门用来处理 **HTTP 请求错误**，比如认证请求失败等。它有如下 3 个属性。

- **`code`**：返回 HTTP **状态码**，比如 404 表示网页不存在，500 表示服务器内部错误
- **`reason`**：同父类一样，用于返回**错误的原因**
- **`headers`**：返回**请求头**

实例：

```
from urllib import request,error
try:
    response = request.urlopen('http://cuiqingcai.com/index.htm')
except error.HTTPError as e:
    print(e.reason, e.code, e.headers, sep='\n')
```

同样的网址，上述代码捕获了 `HTTPError` 异常，运行结果如下：

```
Not Found
404
Server: GitHub.com
Date: Sun, 07 Mar 2021 08:03:02 GMT
Content-Type: text/html; charset=utf-8
X-NWS-UUID-VERIFY: 23fc0dd199c572a639415951bb548552
Access-Control-Allow-Origin: *
ETag: "603c6eb8-c62c"
x-proxy-cache: MISS
X-GitHub-Request-Id: 086C:708D:9551C7:DAEA45:6044879E
Accept-Ranges: bytes
Age: 280
Via: 1.1 varnish
X-Served-By: cache-hnd18721-HND
X-Cache: HIT
X-Cache-Hits: 0
X-Timer: S1615104183.684424,VS0,VE1
Vary: Accept-Encoding
X-Fastly-Request-ID: e7738f6d60aa4a478f71e7e19fd1b3981a4483bd
X-Daa-Tunnel: hop_count=2
X-Cache-Lookup: Hit From Upstream
X-Cache-Lookup: Hit From Inner Cluster
Content-Length: 50732
X-NWS-LOG-UUID: 15716530918793642435
Connection: close
X-Cache-Lookup: Cache Miss
```

因为 `URLError` 是 `HTTPError` 的父类，所以可以<u>先选择捕获子类的错误，再去捕获父类的错误</u>，一个**较好的异常处理代码**如下：

```
from urllib import request, error

try:
    response = request.urlopen('http://cuiqingcai.com/index.htm')
except error.HTTPError as e:
    print(e.reason, e.code, e.headers, sep='\n')
except error.URLError as e:
    print(e.reason)
else:
    print('Request Successfully')
```

- 先捕获 `HTTPError`，获取它的错误状态码、原因、`headers` 等信息
- 如果不是 `HTTPError` 异常，就会捕获 `URLError` 异常，输出错误原因
- 最后，用 `else` 来处理正常的逻辑

有时候，`reason` 属性返回的不一定是字符串，也可能是一个对象。实例：

```
import socket
import urllib.request
import urllib.error

try:
    response = urllib.request.urlopen('https://www.baidu.com', timeout=0.01)
except urllib.error.URLError as e:
    print(type(e.reason))
    print(e.reason)
    if isinstance(e.reason, socket.timeout):
        print('TIME OUT')
```

上述代码直接设置超时时间来强制抛出 `timeout` 异常，运行结果如下：

```
<class 'socket.timeout'>
timed out
TIME OUT
```

可以发现，`reason` 属性的结果是 `socket.timeout` 类。所以，这里我们可以用 **`isinstance()` 方法**来**判断它的类型**，作出更详细的异常判断。

## 3. 解析链接 urllib.parse

urllib 库里还提供了 **`parse` 模块**，它<u>定义了处理 URL 的标准接口</u>，例如实现 URL 各部分的**抽取**、**合并**以及**链接转换**。它支持如下协议的 URL 处理：file、ftp、gopher、hdl、http、https、imap、mailto、 mms、news、nntp、prospero、rsync、rtsp、rtspu、sftp、 sip、sips、snews、svn、svn+ssh、telnet 、wais

### URL的分段 urlparse()

该方法可以实现 URL 的**识别**和**分段**，实例：

```
from urllib.parse import urlparse

result = urlparse('http://www.baidu.com/index.html;user?id=5#comment')
	# 使用 urllib.parse.urlparse() 会报错
print(type(result), result, sep='\n')
```

运行结果如下：

```
<class 'urllib.parse.ParseResult'>
ParseResult(scheme='http', netloc='www.baidu.com', path='/index.html', params='user', query='id=5', fragment='comment')
```

- 结果是一个 **`ParseResult` 类型**的对象，它包含 6 部分
- `scheme`：代表**协议**，即`://`前面的内容；此处为`http`
- `netloc`：代表**域名**，即第一个`/`前的；此处为`www.baidu.com`
- `path`：代表**访问路径**；此处为`/index.html`，注意包含了`/`
- `params`：代表**参数**，即分号`;`后面的；此处为`user`
- `query` ：代表**查询条件**，即问号`?`后面的，易班用作GET类型的URL；此处为`id=5`
- `fragment`：代表**锚点**，即警号`#`后面的；此处即`comment`

返回结果 `ParseResult` 实际上是一个**元组类型**，我们可以用**索引**顺序来获取，也可以用**属性名**获取。示例：

```
>>> print(result.scheme, result.netloc, result.path, result.params, result.query, result.fragment, sep='\n')
http
www.baidu.com
/index.html
user
id=5
comment
>>> print(result[0])
http
```

可以得出一个标准的链接格式，利用 `urlparse()` 方法可以将它拆分开来，具体如下：

```
scheme://netloc/path;parameters?query#fragment
```

除了这种最基本的解析方式外，还有其他配置，`urlparse()` 方法的 **API** 用法：

```
urllib.parse.urlparse(urlstring, scheme='', allow_fragments=True)
```

- **`urlstring`**：这是必填项，即待解析的 **URL**。
- **`scheme`**：它是默认的协议（比如 `http` 或 `https` 等）。只有在`urlstring`链接没有带协议信息时，会将这个作为默认的协议
- **`allow_fragments`**：即是否忽略锚点 `fragment`。如果它被设置为 `False`，`fragment` 部分就会被忽略，它会被解析为 `path`、`parameters` 或者 `query` 的一部分，而 `fragment` 部分为空

### urlunparse()

有了 `urlparse()`，相应地就有了它的对立方法 `urlunparse()`。它接受的参数是一个**可迭代对象**，但是它的**长度必须是 6**，否则会抛出参数数量不足或者过多的问题。实例：

```
from urllib.parse import urlunparse
data = ['http', 'www.baidu.com', 'index.html', 'user', 'a=6', 'comment']  # 路径path没有/也行
print(urlunparse(data))
```

参数 `data` 用了列表类型。当然也可以用其他类型，比如元组或者特定的数据结构。运行结果如下：

```
http://www.baidu.com/index.html;user?a=6#comment
```

### urlsplit()

这个方法和 `urlparse()` 方法非常相似，只不过它不再单独解析 `params` 这一部分，只**返回 5 个结果**。上面例子中的 `params` 会合并到 `path` 中。示例：

```
from urllib.parse import urlsplit
result = urlsplit('http://www.baidu.com/index.html;user?id=5#comment')
print(type(result), result, sep='\n')
```

运行结果如下：

```
<class 'urllib.parse.SplitResult'>
SplitResult(scheme='http', netloc='www.baidu.com', path='/index.html;user', query='id=5', fragment='comment')
```

返回结果是 `SplitResult`，它其实也是一个**元组类型**，既可以用属性获取值，也可以用索引来获取

### urlunsplit()

与 `urlunparse()` 类似，它也是将**链接**各个部分组合成完整链接的方法，传入的参数也是一个可迭代对象，例如列表、元组等，唯一的区别是**长度必须为 5**。示例：

```
from urllib.parse import urlunsplit
data = ['http', 'www.baidu.com', 'index.html', 'a=6', 'comment']
print(urlunsplit(data))
```

运行结果如下：

```
http://www.baidu.com/index.html?a=6#comment
```

### urljoin()

有了 `urlunparse()` 和 `urlunsplit()` 方法，我们可以完成链接的合并，不过前提必须要有特定长度的对象，链接的每一部分都要清晰分开。

此外，**生成链接**还有另一个方法，那就是 **`urljoin()` 方法**。我们可以提供一个 `base_url`（基础链接）作为**第一个参数**，将新的链接作为**第二个参数**，该方法会分析 `base_url` 的 `scheme`、`netloc` 和 `path` 这 3 个内容并**对新链接缺失的部分进行补充**，最后返回结果。

实例：

```
>>> from urllib.parse import urljoin
>>> print(urljoin('http://www.baidu.com', 'FAQ.html'))
http://www.baidu.com/FAQ.html
>>> print(urljoin('http://www.baidu.com', 'https://cuiqingcai.com/FAQ.html'))
https://cuiqingcai.com/FAQ.html
>>> print(urljoin('http://www.baidu.com/about.html', 'https://cuiqingcai.com/FAQ.html'))
https://cuiqingcai.com/FAQ.html
>>> print(urljoin('http://www.baidu.com/about.html', 'https://cuiqingcai.com/FAQ.html?question=2'))
https://cuiqingcai.com/FAQ.html?question=2
>>> print(urljoin('http://www.baidu.com?wd=abc', 'https://cuiqingcai.com/index.php'))
https://cuiqingcai.com/index.php
>>> print(urljoin('http://www.baidu.com', '?category=2#comment'))
http://www.baidu.com?category=2#comment
>>> print(urljoin('www.baidu.com', '?category=2#comment'))
www.baidu.com?category=2#comment
>>> print(urljoin('www.baidu.com#comment', '?category=2'))
www.baidu.com?category=2
```

可以发现，`base_url` 提供了三项内容 `scheme`、`netloc` 和 `path`。如果这 3 项在新的链接里不存在，就予以补充；如果新的链接存在，就使用新的链接的部分。而 `base_url` 中的 `params`、`query` 和 `fragment` 是不起作用的。

通过 `urljoin()` 方法，我们可以轻松实现链接的解析、拼合与生成。

### urlencode()

函数`urlencode()`在将 **字典** 序列化为 **GET请求参数** ，示例：

```
from urllib.parse import urlencode

params = {
    'name': 'germey',
    'age': 22
}  # 用字典表示参数
base_url = 'http://www.baidu.com?'
url = base_url + urlencode(params)  # 调用 urlencode() 方法将params序列化为GET请求参数
print(url)
```

- 首先声明了一个字典来表示参数
- 然后调用 `urlencode()` 方法将其序列化为 GET 请求参数。

不同的请求参数用`&`隔开，运行结果如下：

```
http://www.baidu.com?name=germey&age=22
```

### parse_qs()

有了序列化，必然就有反序列化。利用 **`parse_qs()` 方法**可以将一串  **GET请求参数** 转回 **字典**，示例：

```
from urllib.parse import parse_qs

query = 'name=germey&age=22'
print(type(parse_qs(query)), parse_qs(query))
```

转回为字典类型，运行结果如下：

```
<class 'dict'> {'name': ['germey'], 'age': ['22']}
```

### parse_qsl()

**`parse_qsl()` 方法**将 **参数** 转化为 **元组 组成的 列表** ，示例：

```
from urllib.parse import parse_qsl

query = 'name=germey&age=22'
print(type(query), query, sep='\n')
print(type(parse_qsl(query)), parse_qsl(query), sep='\n')
```

运行结果是一个**列表**，而列表中的<u>每一个元素都是一个元组</u>，元组的第一个内容是参数名，第二个内容是参数值，运行结果如下：

```
<class 'str'>
name=germey&age=22
<class 'list'>
[('name', 'germey'), ('age', '22')]
```

### quote()

**`quote()`方法**可以将 **内容** 转化为 **URL编码** 的格式。应用于：URL 中带有<u>中文参数</u>时，有时可能会导致<u>乱码</u>的问题，此时用这个方法可以将<u>中文字符转化为 URL 编码</u>，示例：

```
from urllib.parse import quote

keyword = '壁纸'
url = 'https://www.baidu.com/s?wd=' + quote(keyword)
print(type(url), url, sep='\n')
```

运行结果如下：

```
<class 'str'>
https://www.baidu.com/s?wd=%E5%A3%81%E7%BA%B8
```

### unquote()

**`unquote()` 方法**可以进行 URL **解码**，示例：

```
from urllib.parse import unquote

url = 'https://www.baidu.com/s?wd=%E5%A3%81%E7%BA%B8'
print(unquote(url))
```

得到的 URL 编码后的结果，运行结果如下：

```
https://www.baidu.com/s?wd=壁纸
```

## 4. 分析Robots协议 urllib.robotparser

利用 urllib 的 **`robotparser` 模块**，我们可以实现网站 Robots 协议的分析。本节中，我们来简单了解一下该模块的用法。

### Robots协议

**Robots 协议**也称作爬虫协议、机器人协议，它的全名叫作**网络爬虫排除标准**（Robots Exclusion Protocol），用来告诉爬虫和搜索引擎<u>哪些页面可以抓取，哪些不可以抓取</u>。它通常是一个叫作 robots.txt 的文本文件，一般放**在网站的根目录**下。

当搜索爬虫访问一个站点时，它首先会检查这个站点根目录下是否存在 robots.txt 文件；**如果存在**，搜索爬虫会根据其中定义的爬取范围来爬取。如果**没有**找到这个文件，搜索爬虫便会访问所有可直接访问的页面。

robots.txt 的样例：

```
User-agent: *
Disallow: /
Allow: /public/
```

这实现了对所有搜索爬虫**只允许爬取 public 目录**的功能，将上述内容保存成 robots.txt 文件，放在网站的根目录下，和网站的**入口文件**（比如 index.php、index.html 和 index.jsp 等）放在一起。

-  **`User-agent`** 描述了搜索**爬虫的名称**，这里将其**设置为 *** 则代表该协议**对任何爬取爬虫有效**。比如，我们可以设置：

  ```
  User-agent: Baiduspider
  ```

  这表示设置的规则对百度爬虫是有效的。如果有多条 `User-agent` 记录，则就会有多个爬虫会受到爬取限制，但至少需要指定一条。

- `Disallow` 指定了**不允许抓取的目录**，比如上例子中**设置为 /** 则代表**所有页面不允许抓取**。

- `Allow` 一般和 `Disallow` 一起使用，一般不会单独使用，用来排除某些限制

现在我们设置为 `/public/`，则表示**所有页面不允许抓取**，**但可以抓取 public 目录**

**/trap链接**是**禁止链接**，如果访问了这个链接，服务器就会**封禁**你的IP一分钟或者永久封禁

**所有用户**，都应该在两次下载请求之间有5秒的时延；/trap链接是禁止链接，如果访问了这个链接，服务器就会封禁你的IP一分钟或者永久封禁 

```
 User-agent: *
 Crawl-delay: 5
 Disallow: /trap
```

**禁止 所有爬虫 访问任何目录**的代码如下：

```
User-agent: * 
Disallow: /
```

**允许 所有爬虫 访问任何目录**的代码如下，或直接把 robots.txt **文件留空**：

```
User-agent: *
Disallow:
```

**禁止 所有爬虫 访问网站 某些目录**的代码如下：

```
User-agent: *
Disallow: /private/
Disallow: /tmp/
```

**只允许 某一个爬虫 访问**的代码如下：

```
User-agent: WebCrawler
Disallow:

User-agent: *
Disallow: /
```

### robotparser.RobotFileParser()

了解 Robots 协议后，可以**使用 `robotparser` 模块解析 robots.txt**，该模块提供了一个**`类RobotFileParser`**，它可以根据某网站的 robots.txt 文件来判断一个爬取爬虫是否有权限来爬取这个网页。

使用该类时只需要在构造方法里传入 robots.txt 的链接即可，`RobotFileParser`类的声明：

```
urllib.robotparser.RobotFileParser(url='')
```

也可以在声明时不传入，此时默认为空；最后再使用 `set_url()` 方法设置

- **`set_url()`**：用来**设置 robots.txt 文件的链接**。如果在创建 `RobotFileParser` 对象时传入了链接，那么就不需要再使用这个方法设置了
- **`read()`**：**读取** robots.txt 文件并进行**分析**。注意，这个方法执行一个读取和分析操作，如果不调用这个方法，接下来的判断都会为 `False`，所以一定记得调用这个方法。这个方法<u>不会返回任何内容</u>，但是**执行了读取操作**
- **`parse()`**：用来**解析** robots.txt 文件，传入的**参数**是 robots.txt 某些行的内容，它会按照 robots.txt 的语法规则来分析这些内容
- **`can_fetch()`**：该方法**传入两个参数**，第一个是 `User-agent`，第二个是要抓取的 URL。**返回**的内容是该搜索引擎是否可以抓取这个 URL，返回结果是 `True` 或 `False`
- **`mtime()`**：返回的是**上次抓取和分析(即read)** robots.txt 的**时间**，这对于长时间分析和抓取的搜索爬虫是很有必要的，你可能需要定期检查来抓取最新的 robots.txt
- **`modified()`**：它同样对长时间分析和抓取的搜索爬虫很有帮助，将<u>当前时间设置为上次抓取和分析 robots.txt 的时间</u>

以简书为例：

```
from urllib.robotparser import RobotFileParser

rp = RobotFileParser()  # 创建RobotFileParser对象
rp.set_url('http://www.jianshu.com/robots.txt')  # 通过 set_url() 方法设置了 robots.txt 的链接
rp.read()
print(rp.can_fetch('*', 'http://www.jianshu.com/p/b67554025d7d'))  # 判断了该网页是否可以被抓取
print(rp2.can_fetch('Mediapartners-Google', 'http://www.jianshu.com/p/b67554025d7d'))
print(rp.can_fetch('*', "http://www.jianshu.com/search?q=python&page=1&type=collections"))
```

- 首先创建 `RobotFileParser` 对象

- 然后通过 `set_url()` 方法设置了 robots.txt 的链接。也可以在声明时直接用如下方法设置：

  ```
  rp = RobotFileParser('http://www.jianshu.com/robots.txt')
  ```

- 然后利用**read()方法**进行读取和分析

- 接着利用 **`can_fetch()` 方法**判断了该网页是否可以被抓取，返回 `True` 或 `False`。运行结果：

  ```
  False
  False
  False
  ```

  结果有误，应该是robot.txt的问题

- 也可以使用 **`parse()` 方法**(代替read方法)执行读取和分析，返回 `True` 或 `False`，示例：

  ```
  rp.parse(urlopen('http://www.jianshu.com/robots.txt').read().decode('utf-8').split('\n'))
  ```

  **上面代码报错原因**：用urllib.request.urlopen()方式打开一个URL，服务器只会收到一个单纯的对于这个页面访问的请求，但是服务器不知道这个请求使用浏览器、操作系统等信息，而缺失这些信息的访问往往是非正常访问，会被一些网站禁止掉。

  **解决办法**：利用`urllib.request.Request()`在请求头`headers`中加入`UserAgent`信息

  ```
  from urllib.robotparser import RobotFileParser
  from urllib.request import urlopen, Request
  
  rp = RobotFileParser()
  headers = {
  	'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'
  }
  request = Request('http://www.jianshu.com/robots.txt', headers=headers)
  rp.parse(urlopen(request).read().decode('utf-8').split('\n'))
  
  print(rp.can_fetch('*', 'http://www.jianshu.com/p/b67554025d7d'))
  print(rp.can_fetch('*', "http://www.jianshu.com/search?q=python&page=1&type=collections"))
  print(rp.can_fetch('Mediapartners-Google', "http://www.jianshu.com/search?q=python&page=1&type=collections"))
  ```

  运行结果正确：

  ```
  True
  False
  True
  ```

  